{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.24.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of qiskit-nature: .* suffix can only be used with `==` or `!=` operators\n",
      "    qiskit-terra (>=0.22.*)\n",
      "                  ~~~~~~~^\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iskit-terra (c:\\users\\niels\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2143 models\n"
     ]
    }
   ],
   "source": [
    "api = HfApi()\n",
    "api_result = api.list_models(cardData=True, full=False, tags=\"co2_eq_emissions\")\n",
    "models = list(api_result)\n",
    "print(f\"Found {len(models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get models' co2 emissions\n",
    "Only return the co2 if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emissions(model):\n",
    "    if not model.card_data:\n",
    "        return None\n",
    "    if not 'co2_eq_emissions' in model.card_data:\n",
    "        return None\n",
    "    return model.card_data['co2_eq_emissions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model data in a dictionary\n",
    "# Keys: {'on_cloud', 'source', 'emissions', 'training_type', 'hours_used', 'cpu_model', 'energy_consumed', 'ram_total_size', 'gpu_model', 'hardware_used', 'geographical_location'}\n",
    "def get_model_data(model):\n",
    "    emissions = get_emissions(model)\n",
    "    emissions_is_dict = isinstance(emissions, dict)\n",
    "    emissions_value = emissions.get('emissions', None) if emissions_is_dict else emissions\n",
    "    emissions_source = emissions.get('source', None) if emissions_is_dict else None\n",
    "    hardware = emissions.get('hardware_used', None) if emissions_is_dict else None\n",
    "    training_type = emissions.get('training_type', 'pretraining') if emissions_is_dict else 'pretraining'\n",
    "    training_location = emissions.get('geographical_location', None) if emissions_is_dict else None\n",
    "    on_cloud = emissions.get('on_cloud', None) if emissions_is_dict else None\n",
    "    hours_used = emissions.get('hours_used', None) if emissions_is_dict else None\n",
    "    cpu_model = emissions.get('cpu_model', None) if emissions_is_dict else None\n",
    "    energy_consumed = emissions.get('energy_consumed', None) if emissions_is_dict else None\n",
    "    ram_total_size = emissions.get('ram_total_size', None) if emissions_is_dict else None\n",
    "    \n",
    "    return {\n",
    "        'modelId': model.modelId,\n",
    "        'author': model.author,\n",
    "        'model_type': model.pipeline_tag,\n",
    "        'created_at': model.created_at,\n",
    "        'downloads': model.downloads,\n",
    "        'likes': model.likes,\n",
    "        'emissions': emissions_value, #kgCO2eq\n",
    "        'emissions_source': emissions_source,\n",
    "        'hardware': hardware,\n",
    "        'training_type': training_type,\n",
    "        'training_location': training_location,\n",
    "        'on_cloud': on_cloud,\n",
    "        'hours_used': hours_used,\n",
    "        'cpu_model': cpu_model,\n",
    "        'energy_consumed': energy_consumed, #kWh\n",
    "        'ram_total_size': ram_total_size #GB\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get models\n",
    "Only retrieve models with co2 emissions available. About 80% of the models have this available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Van de 2143 modellen op HuggingFace hebben er 2143 CO2 emissie data (100.00%)\n"
     ]
    }
   ],
   "source": [
    "emission_models = []\n",
    "for model in models:\n",
    "    if not model.card_data:\n",
    "        continue\n",
    "    if not 'co2_eq_emissions' in model.card_data:\n",
    "        continue\n",
    "    # if not isinstance(model.card_data['co2_eq_emissions'], dict):\n",
    "    #     continue\n",
    "    emission_models.append(get_model_data(model))\n",
    "\n",
    "print(f\"Van de {len(models)} modellen op HuggingFace hebben er {len(emission_models)} CO2 emissie data ({len(emission_models)/len(models)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(emission_models)\n",
    "df['training_type'] = df['training_type'].str.replace('pretraining', 'pre-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>author</th>\n",
       "      <th>model_type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>emissions</th>\n",
       "      <th>emissions_source</th>\n",
       "      <th>hardware</th>\n",
       "      <th>training_type</th>\n",
       "      <th>training_location</th>\n",
       "      <th>on_cloud</th>\n",
       "      <th>hours_used</th>\n",
       "      <th>cpu_model</th>\n",
       "      <th>energy_consumed</th>\n",
       "      <th>ram_total_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bigscience/bloom</td>\n",
       "      <td>None</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2022-05-19 11:53:33+00:00</td>\n",
       "      <td>6158</td>\n",
       "      <td>4733</td>\n",
       "      <td>24700000.0</td>\n",
       "      <td>Estimating the Carbon Footprint of BLOOM, a 17...</td>\n",
       "      <td>384 A100 80GB GPUs</td>\n",
       "      <td>pre-training</td>\n",
       "      <td>Orsay, France</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>dalle-mini/dalle-mega</td>\n",
       "      <td>None</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>2022-06-28 14:07:04+00:00</td>\n",
       "      <td>75</td>\n",
       "      <td>146</td>\n",
       "      <td>450300.0</td>\n",
       "      <td>MLCo2 Machine Learning Impact calculator</td>\n",
       "      <td>TTPU v3-256</td>\n",
       "      <td>pre-training</td>\n",
       "      <td>East USA</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>eci-io/climategpt-7b-fsg</td>\n",
       "      <td>None</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2023-12-01 17:05:08+00:00</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>265800.0</td>\n",
       "      <td>None</td>\n",
       "      <td>8x NVIDIA H100 HBM</td>\n",
       "      <td>pre-training</td>\n",
       "      <td>Washington, USA</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>eci-io/climategpt-7b-fsc</td>\n",
       "      <td>None</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2023-12-01 17:04:50+00:00</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>262800.0</td>\n",
       "      <td>None</td>\n",
       "      <td>8x NVIDIA H100 HBM</td>\n",
       "      <td>pre-training</td>\n",
       "      <td>Washington, USA</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>ysn-rfd/PersianMind-v1.0-Q4_K_M-GGUF</td>\n",
       "      <td>None</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>2024-08-19 05:30:58+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>232380.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pre-training</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   modelId author       model_type  \\\n",
       "0                         bigscience/bloom   None  text-generation   \n",
       "382                  dalle-mini/dalle-mega   None    text-to-image   \n",
       "2007              eci-io/climategpt-7b-fsg   None  text-generation   \n",
       "2006              eci-io/climategpt-7b-fsc   None  text-generation   \n",
       "2123  ysn-rfd/PersianMind-v1.0-Q4_K_M-GGUF   None  text-generation   \n",
       "\n",
       "                    created_at  downloads  likes   emissions  \\\n",
       "0    2022-05-19 11:53:33+00:00       6158   4733  24700000.0   \n",
       "382  2022-06-28 14:07:04+00:00         75    146    450300.0   \n",
       "2007 2023-12-01 17:05:08+00:00         63      4    265800.0   \n",
       "2006 2023-12-01 17:04:50+00:00         52      6    262800.0   \n",
       "2123 2024-08-19 05:30:58+00:00          4      0    232380.0   \n",
       "\n",
       "                                       emissions_source            hardware  \\\n",
       "0     Estimating the Carbon Footprint of BLOOM, a 17...  384 A100 80GB GPUs   \n",
       "382            MLCo2 Machine Learning Impact calculator         TTPU v3-256   \n",
       "2007                                               None  8x NVIDIA H100 HBM   \n",
       "2006                                               None  8x NVIDIA H100 HBM   \n",
       "2123                                               None                None   \n",
       "\n",
       "     training_type training_location on_cloud  hours_used cpu_model  \\\n",
       "0     pre-training     Orsay, France     None         NaN      None   \n",
       "382   pre-training          East USA     None         NaN      None   \n",
       "2007  pre-training   Washington, USA     None         NaN      None   \n",
       "2006  pre-training   Washington, USA     None         NaN      None   \n",
       "2123  pre-training              None     None         NaN      None   \n",
       "\n",
       "      energy_consumed  ram_total_size  \n",
       "0                 NaN             NaN  \n",
       "382               NaN             NaN  \n",
       "2007              NaN             NaN  \n",
       "2006              NaN             NaN  \n",
       "2123              NaN             NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='emissions', ascending=False, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null emissions: 0\n",
      "Zero emissions: 14\n",
      "modelId              1678\n",
      "author                  0\n",
      "model_type           1654\n",
      "created_at           1678\n",
      "downloads            1678\n",
      "likes                1678\n",
      "emissions            1678\n",
      "emissions_source      166\n",
      "hardware              157\n",
      "training_type        1678\n",
      "training_location      78\n",
      "on_cloud               86\n",
      "hours_used             86\n",
      "cpu_model              86\n",
      "energy_consumed        42\n",
      "ram_total_size         86\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by='emissions', ascending=False, inplace=True)\n",
    "\n",
    "# models with emissions data\n",
    "df_emmission = df[df['emissions'].notnull() & df['emissions'] > 0]\n",
    "\n",
    "print(f\"Null emissions: {len(df[df['emissions'].isnull()])}\")\n",
    "print(f\"Zero emissions: {len(df[df['emissions'] == 0])}\")\n",
    "\n",
    "# info about model data fields\n",
    "print(df_emmission.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emission source tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "codecarbon                                                                                                                                                                                                                                                                                                                          88\n",
       "CodeCarbon                                                                                                                                                                                                                                                                                                                          29\n",
       "code carbon                                                                                                                                                                                                                                                                                                                         10\n",
       "https://mlco2.github.io/impact/#compute                                                                                                                                                                                                                                                                                              8\n",
       "Calculated using the [ML CO2 impact calculator](https://mlco2.github.io/impact/#compute), training for 4 x 45 hours with a carbon efficiency of 0.029 kg/kWh. You can inspect the carbon efficiency of the French national grid provider here: https://www.rte-france.com/eco2mix/les-emissions-de-co2-par-kwh-produit-en-france     6\n",
       "MLCo2 Machine Learning Impact calculator                                                                                                                                                                                                                                                                                             4\n",
       "Lacoste, Alexandre, et al. “Quantifying the Carbon Emissions of Machine Learning.” ArXiv (Cornell University), 21 Oct. 2019, https://doi.org/10.48550/arxiv.1910.09700.                                                                                                                                                              3\n",
       "Google Colab                                                                                                                                                                                                                                                                                                                         3\n",
       "calculated using this webstie https://mlco2.github.io/impact/#compute                                                                                                                                                                                                                                                                2\n",
       "https://mlco2.github.io/impact/                                                                                                                                                                                                                                                                                                      1\n",
       "https://mlco2.github.io/impact                                                                                                                                                                                                                                                                                                       1\n",
       "ML CO2 Impact (https://mlco2.github.io/impact/#compute)                                                                                                                                                                                                                                                                              1\n",
       "ML CO2 Impact                                                                                                                                                                                                                                                                                                                        1\n",
       "Quantifying the Carbon Emissions of Machine Learning https://mlco2.github.io/impact#compute                                                                                                                                                                                                                                          1\n",
       "mlco2                                                                                                                                                                                                                                                                                                                                1\n",
       "ML CO2 Impact https://mlco2.github.io/impact/#home)                                                                                                                                                                                                                                                                                  1\n",
       "https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2                                                                                                                                                                                                                                                                        1\n",
       "Google Cloud Platform Carbon Footprint                                                                                                                                                                                                                                                                                               1\n",
       "estimated by using ML CO2 Calculator                                                                                                                                                                                                                                                                                                 1\n",
       "from AutoTrain, code carbon                                                                                                                                                                                                                                                                                                          1\n",
       "PersianMind: A Cross-Lingual Persian-English Large Language Model. https://arxiv.org/abs/2401.06466                                                                                                                                                                                                                                  1\n",
       "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. https://arxiv.org/abs/2211.02001                                                                                                                                                                                                                          1\n",
       "Name: emissions_source, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove None values\n",
    "emission_sources = df_emmission['emissions_source'].dropna()\n",
    "\n",
    "emission_sources.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Evaluation Results\n",
    "[EvalResult](https://huggingface.co/docs/huggingface_hub/v0.25.0.rc0/en/package_reference/cards#huggingface_hub.EvalResult) (HuggingFace Hub API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_details(model):\n",
    "    if not model.card_data:\n",
    "        return None\n",
    "    if not 'eval_results' in model.card_data or model.card_data['eval_results'] is None:\n",
    "        return None\n",
    "    \n",
    "    eval_results = model.card_data['eval_results']\n",
    "    res = []\n",
    "\n",
    "    for evaluation in eval_results:\n",
    "        eval_details = {}\n",
    "        for key, value in vars(evaluation).items():\n",
    "            if value is not None:\n",
    "              eval_details[key] = value\n",
    "        \n",
    "        res.append(eval_details)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/2124 models have evaluation results and 1659 evaluations in total (avg 15.50 per model)\n",
      "107/2124 models have CO2 emissions and evaluation results\n"
     ]
    }
   ],
   "source": [
    "models_with_eval = 0\n",
    "evals = 0\n",
    "co2_and_evals = 0\n",
    "for model in models:\n",
    "  get_eval_details(model)\n",
    "  if get_eval_details(model):\n",
    "    models_with_eval += 1\n",
    "    evals += len(get_eval_details(model))\n",
    "    if get_emissions(model):\n",
    "        co2_and_evals += 1\n",
    "\n",
    "print(f\"{models_with_eval}/{len(models)} models have evaluation results and {evals} evaluations in total (avg {evals/models_with_eval:.2f} per model)\")\n",
    "print(f\"{co2_and_evals}/{len(models)} models have CO2 emissions and evaluation results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['region', 'other', 'library', 'license', 'language', 'dataset', 'pipeline_tag'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = requests.get(\n",
    "  \"https://huggingface.co/api/models-tags-by-type\",\n",
    "  params={},\n",
    "  headers={}\n",
    ").json()\n",
    "\n",
    "# get all keys\n",
    "keys = tags.keys()\n",
    "keys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
